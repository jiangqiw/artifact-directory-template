Title:
Balancing Accuracy and Efficiency: A Comparative Study of Knowledge Distillation and Post-Training Quantization Sequences

Abstract:
This study examines the interplay between knowledge distillation and post-training quantization techniques to optimize deep neural networks (DNNs) for deployment in resource-constrained environments. We systematically evaluate various distillation methods—including Vanilla Knowledge Distillation (VKD), Mixup augmentation, Deep Mutual Learning (DML), and Decoupled Knowledge Distillation (DKD)—applied to compressing ResNet50 models into smaller ResNet18 models. Post-training quantization using a greedy path-following algorithm further reduces model size and computational load. Experimental results on CIFAR-10 and CIFAR-100 datasets demonstrate that, while distilled student models effectively retain accuracy at moderate quantization levels (around 8-bit), accuracy sharply declines at lower bit widths, especially on complex datasets like CIFAR-100. This marks student model are less compressible at lower bits, the model compression strategy for combining knowledge distillation with quantization is still significant with highly-constrained deployment environment and dealing with less complex tasks.

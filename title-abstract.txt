Title:
Balancing Accuracy and Efficiency: A Comparative Study of Knowledge Distillation and Post-Training Quantization Sequences

Abstract:
This study examines the interplay between knowledge distillation and post-training quantization techniques to optimize deep neural networks (DNNs) for deployment in resource-constrained environments. We systematically evaluate various distillation methods—including Vanilla Knowledge Distillation (VKD), Mixup augmentation, Deep Mutual Learning (DML), and Decoupled Knowledge Distillation (DKD)—applied to compressing ResNet50 models into smaller ResNet18 models. Post-training quantization using a greedy path-following algorithm further reduces model size and computational load. Experimental results on CIFAR-10 and CIFAR-100 datasets demonstrate that, while distilled student models effectively retain accuracy at moderate quantization levels (around 8-bit), accuracy sharply declines at lower bit widths, especially on complex datasets like CIFAR-100. Interestingly, on the simpler CIFAR-10 dataset, certain distilled students (e.g., Mixup) surpass the teacher's performance at comparable sizes. Our findings highlight critical considerations for deploying compressed models in practice, emphasizing the trade-offs between model accuracy and efficiency across different dataset complexities.
